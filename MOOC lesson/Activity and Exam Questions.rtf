{\rtf1\ansi\ansicpg1252\cocoartf1348\cocoasubrtf170
{\fonttbl\f0\fswiss\fcharset0 Helvetica;}
{\colortbl;\red255\green255\blue255;}
\paperw11900\paperh16840\margl1440\margr1440\vieww12600\viewh13200\viewkind0
\pard\tx566\tx1133\tx1700\tx2267\tx2834\tx3401\tx3968\tx4535\tx5102\tx5669\tx6236\tx6803\pardirnatural

\f0\fs24 \cf0 1. Install the imageFilters package using the WEKA package manager.\
\
2. Open the data folder in the package. You will find a folder called vehicle_images that contains the images for this activity.\
\
3. Choose the PHOGFilter from the unsupervised/instances/imageFilters menu and filter the image dataset set. (Note: you will have to make sure that the imageDirectory property of the filter is set correctly, or WEKA won\'92t be able to find the images!)\
\
	
\b Q1: how many features are extracted?
\b0  \
	Choices\
	\'97 629 \
	\'97 630 (correct answer)\
	\'97 631 \
	\'97 1024\
\
4. Remove the first string attribute and then run three 10 fold CV experiments on the filtered dataset using J48, Naive Bayes and SMO.\
\
	
\b Q2: which classifier has the highest accuracy and what is the highest accuracy?
\b0  \
	Choices:\
	\'97 SMO, 78.1%\
	\'97 J48, 82.3%\
	\'97 Naive Bayes, 73.33% (correct answer)\
	\'97 SMO and J48 tie at 75%\
	\
\
5. Undo the changes to the dataset and then repeat steps 4 and 5, but this time use the ColorLayout filter.\
\
	
\b Q3: which classifier has the highest accuracy and what is the highest accuracy now?
\b0  \
	Choices:\
	\'97 Naive Bayes and SMO tie at 71.67% (correct answer)\
	\'97 Naive Bayes, 99%\
	\'97 SMO, 72.3%\
	\'97 Naive Bayes and J48 tie at 75%\
\
6. Undo the changes again, and this time apply BOTH the EdgeHistogram Filter and the ColorLayout Filter in sequence, so that the current dataset contains both sets of features. Rerun your experiments.\
\
	
\b Q4: what is the best classifier/accuracy now? 
\b0 \
	Choices:\
	\'97 Naive Bayes , 86.67%\
	\'97 J48, 61.67%\
	\'97 Naive Bayes and SMO tie at 86.67%\
	\'97 SMO, 91.67% (correct)\
\
	
\b Q5: of the three classes, which has the least misclassifications when SMO is used? 
\b0 \
	Choices:\
	\'97 TRAIN class (correct)\
	\'97 PLANE class\
	\'97 CAR class\
	\'97 TRAIN and PLANE tie  \
\
7. Its interesting to look at the usefulness of the image features. With your EdgeHistogram/ColorLayout dataset still open, go to \'93Select Attributes\'94 and use the InfoGainAttributeEval/Ranker combination to rank the attributes.\
\
	
\b Q6: One type of feature is clearly the most useful. Which one?
\b0  \
	Choices\
	\'97 the colour features\
	\'97 the edge features (correct)\
	\'97 both are equally important\
	\'97 neither are important\
\
	
\b Q7: Look at the images in the dataset. Is there a reason for your answer to Q6?
\b0  \
	Choices\
 	\'97 colour describes the distinct appearance of the cars/planes/trains but there are few distinctive shapes or edges within each class\
	\'97 edges describe the distinct shapes of the cars/planes/trains but there are few distinctive colours within each class (correct)\
	\'97 the features are statistical and unrelated to the images; therefore neither of the feature types are very useful\
\
\

\b Possible exam questions\

\b0 \
What is an image feature?\
Choices\
\'97 a statistic describing the size of an image, such as its width\
\'97 a statistic describing how many times an image has been viewed\
\'97 a statistic describing one aspect of an image, such as the number of pixels of certain colour (correct)\
\'97 the bit data of a single pixel \
\
What features would most likely distinguish between the two classes of leaf from different species of tree?\
Choices\
\'97 colour-based features\
\'97 shape-based features (correct)\
\'97 neither\
\'97 both are equally likely to do well\
\
}